{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/ai-rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup prerequisites\n",
    "\n",
    "Replace `<MONGODB_URI>` with your **MongoDB connection string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain the quotes (\"\") when pasting the URI\n",
    "MONGODB_URI = os.environ.get(\"MONGODB_URI\")\n",
    "# Initialize a MongoDB Python client\n",
    "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
    "# Check the connection to the server\n",
    "mongodb_client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVERLESS_URL = os.environ.get(\"SERVERLESS_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vernica/proj/mongodb/genai-devday-notebooks/vector-search-lab/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# You may see a warning upon running this cell. You can ignore it.\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 20 examples [00:00, 7443.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the `mongodb-docs` dataset from Hugging Face\n",
    "data = load_dataset(\"mongodb/mongodb-docs\", split=\"train\")\n",
    "# Convert the dataset into a dataframe first, then into a list of Python objects/dictionaries\n",
    "docs = pd.DataFrame(data).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the number of documents in the dataset\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp\\n\\n</td>\\n<td headers=\"Description\">\\nThe date and time of the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nUsername\\n\\n</td>\\n<td headers=\"Description\">\\nThe username associated with the database user who made the authentication request.\\n\\nFor LDAP usernames, the UI displays the resolved LDAP name. Hover over the name to see the full LDAP username.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nIP Address\\n\\n</td>\\n<td headers=\"Description\">\\nThe IP address of the machine that sent the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nHost\\n\\n</td>\\n<td headers=\"Description\">\\nThe target server that processed the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Source\\n\\n</td>\\n<td headers=\"Description\">\\nThe database that the authentication request was made against. `admin` is the authentication source for SCRAM-SHA users and `$external` for LDAP users.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Result\\n\\n</td>\\n<td headers=\"Description\">\\nThe success or failure of the authentication request. A reason code is displayed for the failed authentication requests.\\n\\n</td>\\n</tr>\\n</table>Authentication requests are pre-sorted by descending timestamp with 25 entries per page.\\n\\n### Logging Limitations\\n\\nIf a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time.\\n\\nLog analysis rate limits apply only to the Performance Advisor UI, the Query Insights UI, the Access Tracking UI, and the Atlas Search Query Analytics UI. Downloadable log files are always complete.\\n\\nIf authentication requests occur during a period when logs are not collected, they will not appear in the database access history.\\n\\n## Required Access\\n\\nTo view database access history, you must have `Project Owner` or `Organization Owner` access to Atlas.\\n\\n## Procedure\\n\\n<Tabs>\\n\\n<Tab name=\"Atlas CLI\">\\n\\nTo return the access logs for a cluster using the Atlas CLI, run the following command:\\n\\n```sh\\n\\natlas accessLogs list [options]\\n\\n```\\n\\nTo learn more about the command syntax and parameters, see the Atlas CLI documentation for atlas accessLogs list.\\n\\n- Install the Atlas CLI\\n\\n- Connect to the Atlas CLI\\n\\n</Tab>\\n\\n<Tab name=\"Atlas Administration API\">\\n\\nTo view the database access history using the API, see Access Tracking.\\n\\n</Tab>\\n\\n<Tab name=\"Atlas UI\">\\n\\nUse the following procedure to view your database access history using the Atlas UI:\\n\\n### Navigate to the Clusters page for your project.\\n\\n- If it is not already displayed, select the organization that contains your desired project from the  Organizations menu in the navigation bar.\\n\\n- If it is not already displayed, select your desired project from the Projects menu in the navigation bar.\\n\\n- If the Clusters page is not already displayed, click Database in the sidebar.\\n\\n### View the cluster\\'s database access history.\\n\\n- On the cluster card, click .\\n\\n- Select View Database Access History.\\n\\nor\\n\\n- Click the cluster name.\\n\\n- Click .\\n\\n- Select View Database Access History.\\n\\n</Tab>\\n\\n</Tabs>\\n\\n',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document to understand its structure\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common list of separators for text data\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `RecursiveCharacterTextSplitter` from LangChain to first split a piece of text on the list of `separators` above.\n",
    "# Then recursively merge them into tokens until the specified chunk size is reached.\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk.\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended to maintain context between chunks.\n",
    "# Pass the `separators` list above as an argument called `separators`\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\", separators=separators, chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunk up a document.\n",
    "\n",
    "    Args:\n",
    "        doc (Dict): Parent document to generate chunks from.\n",
    "        text_field (str): Text field to chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    # Extract the field to chunk from `doc`\n",
    "    text = doc[text_field]\n",
    "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
    "    # NOTE: `text` is a string\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Iterate through `chunks` and for each chunk:\n",
    "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
    "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
    "    # 3. Append `temp` to `chunked_data`\n",
    "    chunked_data = []\n",
    "    for chunk in chunks:\n",
    "        temp = doc.copy()\n",
    "        temp[text_field]=chunk\n",
    "        chunked_data.append(temp)\n",
    "\n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []\n",
    "# Iterate through `docs`, use the `get_chunks` function to chunk up the \"body\" field in the documents, and add the list of chunked documents to `split_docs` initialized above.\n",
    "for doc in docs:\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    split_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that the length of `split_docs` is greater than the length of `docs` from Step 2 above\n",
    "# This is because each document in `docs` has been split into multiple chunks\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a chunked document to understand its structure\n",
    "# Note that the structure looks similar to the original docs, except the `body` field now contains smaller chunks of text\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may see a warning upon running this cell. You can ignore it.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the `gte-small` model using the Sentence Transformers library\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/thenlper/gte-small#usage (See \"Use with sentence-transformers\" under Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 24.18it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_docs = []\n",
    "# Add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "for doc in tqdm(split_docs):\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"body\"])\n",
    "    embedded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Ingest data into MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_genai_devday\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `DB_NAME` database.\n",
    "# Use the `mongodb_client` instantiated in Step 1.\n",
    "db = mongodb_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `COLLECTION_NAME` collection.\n",
    "# Use the `db` and collection name defined above.\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff0000000000000024'), 'opTime': {'ts': Timestamp(1740937486, 7), 't': 36}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1740937486, 7), 'signature': {'hash': b'\\x11\\x10\\xaf\\xe9v\\xa1\\xaa1\\\\\\xd5\\xc2\\x90\\xbc\\x0c\\xb6\\xffuC\\xec>', 'keyId': 7448739108320444419}}, 'operationTime': Timestamp(1740937486, 7)}, acknowledged=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 107 documents into the knowledge_base collection.\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(f\"Ingested {collection.count_documents({})} documents into the {COLLECTION_NAME} collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Create a vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index definition specifying:\n",
    "# path: Path to the embeddings field\n",
    "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
    "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vector_index'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector search index with the above definition for the `collection` collection\n",
    "collection.create_search_index(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Perform vector search on your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector search function\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 4\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = [\n",
    "        {\"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5\n",
    "        }},\n",
    "        {\"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }}\n",
    "    ]\n",
    "\n",
    "    # Execute the aggregation `pipeline` and store the results in `results`\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run vector search queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# Backup and Restore Sharded Clusters\\n\\nThe following tutorials describe backup and restoration for sharded clusters:\\n\\nTo use `mongodump` and `mongorestore` as a backup strategy for sharded clusters, you must stop the sharded cluster balancer and use the `fsync` command or the `db.fsyncLock()` method on `mongos` to block writes on the cluster during backups.\\n\\nSharded clusters can also use one of the following coordinated backup and restore processes, which maintain the atomicity guarantees of transactions across shards:\\n\\n- MongoDB Atlas\\n\\n- MongoDB Cloud Manager\\n\\n- MongoDB Ops Manager\\n\\nUse file system snapshots back up each component in the sharded cluster individually. The procedure involves stopping the cluster balancer. If your system configuration allows file system backups, this might be more efficient than using MongoDB tools.\\n\\nCreate backups using `mongodump` to back up each component in the cluster individually.',\n",
       "  'score': 0.9431124925613403},\n",
       " {'body': 'Create backups using `mongodump` to back up each component in the cluster individually.\\n\\nLimit the operation of the cluster balancer to provide a window for regular backup operations.\\n\\nAn outline of the procedure and consideration for restoring an *entire* sharded cluster from backup.',\n",
       "  'score': 0.9327977895736694},\n",
       " {'body': \"# Configuration and Maintenance\\n\\nThis section describes routine management operations, including updating your MongoDB deployment's configuration.\\n\\nOutlines common MongoDB configurations and examples of best-practice configurations for common use cases.\\n\\nUpgrade a MongoDB deployment to a different patch release within the same major release series.\\n\\nStart, configure, and manage running `mongod` process.\\n\\nStop in progress MongoDB client operations using `db.killOp()` and `maxTimeMS()`.\\n\\nArchive the current log files and start new ones.\",\n",
       "  'score': 0.9310877323150635},\n",
       " {'body': '## Full Time Diagnostic Data Capture\\n\\nTo help MongoDB engineers analyze server behavior, `mongod` and `mongos` processes include a Full Time Diagnostic Data Capture (FTDC) mechanism. FTDC is enabled by default. Due to its importance in debugging deployments, FTDC thread failures are fatal and stop the parent `mongod` or `mongos` process.\\n\\nFTDC data files are compressed and not human-readable. They inherit the same file access permissions as the MongoDB data files. Only users with access to FTDC data files can transmit the FTDC data.\\n\\nMongoDB engineers cannot access FTDC data without explicit permission and assistance from system owners or operators.\\n\\nFTDC data **never** contains any of the following information:\\n\\n- Samples of queries, query predicates, or query results\\n\\n- Data sampled from any end-user collection or index\\n\\n- System or MongoDB user credentials or security certificates',\n",
       "  'score': 0.9286065697669983},\n",
       " {'body': \"# Create a MongoDB Deployment\\n\\nYou can create a free tier MongoDB deployment on MongoDB Atlas to store and manage your data. MongoDB Atlas hosts and manages your MongoDB database in the cloud.\\n\\n## Create a Free MongoDB deployment on Atlas\\n\\nComplete the Get Started with Atlas guide to set up a new Atlas account and load sample data into a new free tier MongoDB deployment.\\n\\n## Save your Credentials\\n\\nAfter you create your database user, save that user's username and password to a safe location for use in an upcoming step.\\n\\nAfter you complete these steps, you have a new free tier MongoDB deployment on Atlas, database user credentials, and sample data loaded in your database.\\n\\nIf you run into issues on this step, ask for help in the MongoDB Community Forums or submit feedback by using the Rate this page tab on the right or bottom right side of this page.\",\n",
       "  'score': 0.9270492792129517}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '### Query Targeting\\n\\nQuery Targeting alerts often indicate inefficient queries.\\n\\nTo learn more, see Fix Query Issues.\\n\\n### Connection Limits\\n\\nConnection alerts typically occur when the maximum number of allowable connections to a MongoDB process has been exceeded. Once the limit is exceeded, no new connections can be opened until the number of open connections drops down below the limit.\\n\\nTo learn more, see Fix Connection Issues.\\n\\n## Alerts Workflow\\n\\nWhen an alert condition is met, the alert lifecycle begins.\\n\\nTo learn more, see the Alerts Workflow.',\n",
       "  'score': 0.9637913703918457},\n",
       " {'body': '# Resolve Alerts\\n\\nAtlas issues alerts for the database and server conditions configured in your alert settings. When a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Your alert settings determine the notification methods. Atlas continues sending notifications at regular intervals until the condition resolves or you delete or disable the alert. You should fix the immediate problem, implement a long-term solution, and view metrics to monitor your progress.\\n\\nIf you integrate with VictorOps, OpsGenie, or DataDog, you can recieve informational alerts from these third-party monitoring services in Atlas. However, you must resolve these alerts within each external service.\\n\\n<Tabs>\\n\\n<Tab name=\"Organization Alerts\">\\n\\n</Tab>\\n\\n<Tab name=\"Project Alerts\">\\n\\n</Tab>\\n\\n</Tabs>\\n\\n## View Alerts\\n\\n<Tabs>\\n\\n<Tab name=\"Organization Alerts\">',\n",
       "  'score': 0.9455000162124634},\n",
       " {'body': 'To configure these alert conditions, see Configure Alert Settings.\\n\\n### CPU Steal\\n\\nConfigure the alert settings to send an alert if this metric rises above 10%.\\n\\nTo configure this alert condition, see Configure Alert Settings.\\n\\n### Query Targeting\\n\\nConfigure the alert settings to send an alert if this metric rises above 50 or 100.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n### Connection Limits\\n\\nConfigure the alert settings to send an alert if the Connection % of the configured limit rises above 80% or 90%.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n## Resolve Alerts\\n\\nWhen a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Resolve these alerts and work to prevent alert conditions from occurring in the future. To learn how to fix the immediate problem, implement a long-term solution, and monitor your progress, see Resolve Alerts.\\n\\n### Tickets Available',\n",
       "  'score': 0.9340318441390991},\n",
       " {'body': '# Alert Basics\\n\\nAtlas provides built-in tools, alerts, charts, integrations, and logs to help you monitor your clusters. Atlas provides alerts to help you monitor your clusters and improve performance in the following ways:\\n\\n1. A variety of conditions can trigger an alert.\\n\\n2. You can configure alerts settings based on specific conditions for your databases, users, accounts, and more.\\n\\n3. When you resolve alerts, you can fix the immediate problem, implement a long-term solution, and monitor your progress.\\n\\nAtlas issues alerts for the database and server conditions configured in your alert settings. When a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Your alert settings determine the notification methods. Atlas continues sending notifications at regular intervals until the condition resolves or you delete or disable the alert.\\n\\n## Useful Metrics and Alert Conditions',\n",
       "  'score': 0.9327150583267212},\n",
       " {'body': 'To learn more, see the Connection alert conditions.\\n\\n## Configure Alerts\\n\\nTo set which conditions trigger alerts and how users are notified, Configure Alert Settings. You can configure alerts at the organization or project level. Atlas provides default alerts at the project level. You can clone existing alerts and configure maintenance window alerts.\\n\\nExperiment with alert condition values based on your specific requirements. Periodically reassess these values for optimal performance.\\n\\n### Tickets Available\\n\\nConfigure the alert settings to send an alert if these metrics drop below 30 for at least a few minutes. You want to avoid false positives triggered by relatively harmless short-term drops, but catch issues when these metrics stay low for a while.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n### Queues\\n\\nConfigure the alert settings to send an alert if these metrics rise above 100 for a minute. You want to avoid false positives triggered by relatively harmless short-term spikes, but catch issues when these metrics stay elevated for a while.',\n",
       "  'score': 0.9326194524765015}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"How to resolve alerts in MongoDB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 🦹‍♀️ Combine pre-filtering with vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for documents where the product name is `MongoDB Atlas`\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include the `metadata.productName` field as a `filter` field\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filter\",\n",
    "                \"path\": \"metadata.productName\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vector search index\n",
    "collection.update_search_index(\n",
    "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Check that the index update is complete before proceeding. To do so, ensure that the status of the index shows \"Ready\" in the Atlas UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.productName` field has the value \"MongoDB Atlas\"\n",
    "pipeline = [\n",
    "        {\"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\n",
    "                \"metadata.productName\": \"MongoDB Atlas\"\n",
    "            }\n",
    "        }},\n",
    "        {\"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp',\n",
       "  'score': 0.9098455905914307},\n",
       " {'body': '### Query Targeting\\n\\nQuery Targeting alerts often indicate inefficient queries.\\n\\nTo learn more, see Fix Query Issues.\\n\\n### Connection Limits\\n\\nConnection alerts typically occur when the maximum number of allowable connections to a MongoDB process has been exceeded. Once the limit is exceeded, no new connections can be opened until the number of open connections drops down below the limit.\\n\\nTo learn more, see Fix Connection Issues.\\n\\n## Alerts Workflow\\n\\nWhen an alert condition is met, the alert lifecycle begins.\\n\\nTo learn more, see the Alerts Workflow.',\n",
       "  'score': 0.9058851599693298},\n",
       " {'body': '</td>\\n<td headers=\"Description\">\\nIndicates inefficient queries.\\n\\nThe change streams cursors that the Atlas Search process (`mongot`) uses to keep Atlas Search indexes updated can contribute to the query targeting ratio and trigger query targeting alerts if the ratio is high.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nReplica Set Has No Primary\\n\\n</td>\\n<td headers=\"Description\">\\nNo primary is detected in replica set.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nReplication Oplog Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nAmount of oplog data generated on a primary cluster member is larger than the cluster\\'s configured oplog size.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nSystem CPU Usage Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nCPU usage of the MongoDB process reaches a specified threshold.\\n\\n</td>\\n</tr>\\n</table>',\n",
       "  'score': 0.8908547163009644},\n",
       " {'body': '</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nAtlas Search Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nAmount of CPU and memory used by Atlas Search processes reach a specified threshold.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nConnection Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nNumber of connections to a MongoDB process exceeds the allowable maximum.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nDisk Space % Used Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nPercentage of used disk space on a partition reaches a specified threshold.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nQuery Targeting Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nIndicates inefficient queries.',\n",
       "  'score': 0.8889497518539429},\n",
       " {'body': '</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Source\\n\\n</td>\\n<td headers=\"Description\">\\nThe database that the authentication request was made against. `admin` is the authentication source for SCRAM-SHA users and `$external` for LDAP users.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Result\\n\\n</td>\\n<td headers=\"Description\">\\nThe success or failure of the authentication request. A reason code is displayed for the failed authentication requests.\\n\\n</td>\\n</tr>\\n</table>Authentication requests are pre-sorted by descending timestamp with 25 entries per page.\\n\\n### Logging Limitations\\n\\nIf a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time.',\n",
       "  'score': 0.8866615295410156}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on documents which have been updated on or after `2024-05-19` and where the content type is `Tutorial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include `metadata.contentType` and `updated` as `filter` fields\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filter\",\n",
    "                \"path\": \"metadata.contentType\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filter\",\n",
    "                \"path\": \"updated\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vector search index\n",
    "collection.update_search_index(\n",
    "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Check that the index update is complete before proceeding. To do so, ensure that the status of the index shows \"Ready\" in the Atlas UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
    "# the `metadata.contentType` field has the value \"Tutorial\"\n",
    "# AND\n",
    "# the `updated` field is greater than or equal to \"2024-05-19\"\n",
    "pipeline = [\n",
    "        {\"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\n",
    "                \"$and\": [\n",
    "                    {\"metadata.contentType\": \"Tutorial\"},\n",
    "                    {\"updated\": {\"$gte\": \"2024-05-19\"}}\n",
    "                ]\n",
    "            }\n",
    "        }},\n",
    "        {\"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '**Getting the best recommendations**\\n\\nFor best results, follow these practices.\\n\\n - Give CodeWhisperer something to work with. The more code your file contains, the more context CodeWhisperer has for generating recommendations.\\n - Write descriptive comments in natural language — for example\\n```\\n// Take a JSON document as a String and store it in MongoDB returning the _id\\n```\\nOr\\n```\\n//Insert a document in a collection with a given _id and a discountLevel\\n```\\n - Specify the libraries you prefer at the start of your file by using import statements.\\n```\\n// This Java class works with MongoDB sync driver.\\n// This class implements Connection to MongoDB and CRUD methods.\\n```\\n - Use descriptive names for variables and functions\\n - Break down complex tasks into simpler tasks\\n\\n**Provide feedback**\\n----------------',\n",
       "  'score': 0.9205131530761719},\n",
       " {'body': \"# Getting Started with MongoDB and AWS Codewhisperer\\n\\n**Introduction**\\n----------------\\n\\nAmazon CodeWhisperer is trained on billions of lines of code and can generate code suggestions — ranging from snippets to full functions — in real-time, based on your comments and existing code. AI code assistants have revolutionized developers’ coding experience, but what sets Amazon CodeWhisperer apart is that MongoDB has collaborated with the AWS Data Science team, enhancing its capabilities!\\n\\nAt MongoDB, we are always looking to enhance the developer experience, and we've fine-tuned the CodeWhisperer Foundational Models to deliver top-notch code suggestions — trained on, and tailored for, MongoDB. This gives developers of all levels the best possible experience when using CodeWhisperer for MongoDB functions.\",\n",
       "  'score': 0.9131487607955933},\n",
       " {'body': 'const code = responseData.choices[0].message.content;\\n        // Get the required data to be added into the document\\n        const updateDoc = JSON.parse(code)\\n        // Set a flag that this document does not need further re-processing \\n        updateDoc.process = true\\n        await collection.updateOne({_id : docId}, {$set : updateDoc});\\n      \\n\\n    } else {\\n        console.error(\"Failed to generate filter JSON.\");\\n        console.log(JSON.stringify(responseData));\\n        return {};\\n    }\\n};\\n```\\n\\nKey steps include:',\n",
       "  'score': 0.9075397253036499},\n",
       " {'body': '**Provide feedback**\\n----------------\\n\\nAs with all generative AI tools, they are forever learning and forever expanding their foundational knowledge base, and MongoDB is looking for feedback. If you are using Amazon CodeWhisperer in your MongoDB development, we’d love to hear from you. \\n\\nWe’ve created a special “codewhisperer” tag on our [Developer Forums][6], and if you tag any post with this, it will be visible to our CodeWhisperer project team and we will get right on it to help and provide feedback. If you want to see what others are doing with CodeWhisperer on our forums, the [tag search link][7] will jump you straight into all the action. \\n\\nWe can’t wait to see your thoughts and impressions of MongoDB and Amazon CodeWhisperer together.',\n",
       "  'score': 0.9073591232299805},\n",
       " {'body': 'return sampledReviews;\\n}\\n```\\n\\n### Main trigger logic\\n\\nThe main trigger logic is invoked when an update change event is detected with a `\"process\" : false` field.\\n```javascript\\nexports = async function(changeEvent) {\\n  // A Database Trigger will always call a function with a changeEvent.\\n  // Documentation on ChangeEvents: https://www.mongodb.com/docs/manual/reference/change-events\\n\\n  // This sample function will listen for events and replicate them to a collection in a different Database\\nfunction sampleReviews(reviews) {\\n// Logic above...\\n   if (reviews.length <= 50) {\\n        return reviews;\\n    }\\n    const sampledReviews = [];\\n    const seenIndices = new Set();\\n\\n    while (sampledReviews.length < 50) {\\n        const randomIndex = Math.floor(Math.random() * reviews.length);\\n        if (!seenIndices.has(randomIndex)) {\\n            seenIndices.add(randomIndex);\\n            sampledReviews.push(reviews[randomIndex]);\\n        }\\n    }',\n",
       "  'score': 0.9065394401550293}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Build the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create the chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to answer user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to create a chat prompt\n",
    "    prompt = create_prompt(user_query)\n",
    "    # Format the message to the LLM in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "    # Print the final answer\n",
    "    print(response.json()[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes on the cluster during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows it.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster from backup.\n",
      "\n",
      "These practices help ensure consistent and reliable backups of MongoDB sharded clusters.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I DON'T KNOW\n",
      "\n",
      "The given context does not contain any question that you asked me. The context provides information about using CodeWhisperer, requesting code suggestions, and some JSON data related to reviews, but it does not include a previous question from you.\n"
     ]
    }
   ],
   "source": [
    "# Notice that the LLM does not remember the conversation history at this stage\n",
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦹‍♀️ Re-rank retrieved results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a re-ranking step to the following function\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Extract the \"body\" field from each document in `context`\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "    # Use the `rerank_model` instantiated above to re-rank `documents`\n",
    "    # Set the `top_k` argument to 5 \n",
    "    reranked_documents = rerank_model.rank(\n",
    "        user_query,\n",
    "        documents,\n",
    "        return_documents=True,\n",
    "        top_k=5\n",
    "    )\n",
    "    # Join the re-ranked documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([d.get(\"text\", \"\") for d in reranked_documents])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "# Note the impact of re-ranking on the generated answer\n",
    "# You might not see a difference in this example since we are only re-ranking 5 documents\n",
    "# In practice, you would send a larger number of documents to the re-ranker, and get the top few AFTER reranking\n",
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Add memory to the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_collection = mongodb_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "history_collection.create_index('session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to store chat messages in MongoDB\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = {\n",
    "        \"session_id\": session_id,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now(),\n",
    "    }\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    history_collection.insert_one(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to retrieve chat history from MongoDB\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    if cursor:\n",
    "        # Iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle chat history in the `generate_answer` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"body\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    message_history = retrieve_session_history(session_id)\n",
    "    messages.extend(message_history)\n",
    "\n",
    "    # Format the user query in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    user_message = {\"role\": \"user\", \"content\": user_query}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response.json()[\"text\"]\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    store_chat_message(session_id, \"user\", user_query)\n",
    "    store_chat_message(session_id, \"assistant\", answer)\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the fsync command or db.fsyncLock() method on mongos to block writes during backups when using mongodump and mongorestore.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain transaction atomicity across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component of the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using mongodump to back up each component of the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full cluster backups, following a specific procedure to restore the entire sharded cluster from backup.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are some best practices for data backups in MongoDB?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion in my previous response. You asked:\n",
      "\n",
      "\"What are some best practices for data backups in MongoDB?\"\n",
      "\n",
      "However, after reviewing the given context more carefully, I realize that the context does not contain any information about MongoDB backup best practices. The context mainly discusses using CodeWhisperer and some details about MongoDB reviews and API interactions. \n",
      "\n",
      "Given that there is no relevant information in the provided context to answer your specific question about MongoDB backup best practices, the correct response should have been:\n",
      "\n",
      "I DON'T KNOW\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
